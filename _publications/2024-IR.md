---
title: "Towards Single Sand-Dust Image Restoration via Vision Transformer with Multi-Scale Feature Aggregation"
collection: publications
permalink: /publication/IR
date: 2024-12-24
venue: 'IEEE 21st India Council International Conference (INDICON)'
---
Authors: **Romala Mishra**, Sobhan Kanti Dhara, Anusha Vupputuri, Sukhadev Meher

Sand-dust images contain suspended dust particles that degrade the image quality in terms of visibility and, illumination leading to hazy images, distorted depth perception, color shifts, and wavelength scattering causing tints in images which impacts various vision based tasks. Many existing Sand-dust restoration techniques struggle to preserve fine details during restoration and face difficulty in varying Sand-dust conditions. To address these challenges, we propose a vision transformer-based framework that employs shifted window attention and depth-wise convolution for efficient local attention computation. It incorporates layer normalization that is revised to enhance stability and is integrated with a multi-scale feature aggregation module that allows the framework to address varying levels of haze by balancing the extraction of global contextual information with the preservation of local details and reducing overhead computation. Overall, our framework is found to not only effectively restore varying Sand-dust conditions as compared to other state-of-the-art frameworks but also address color shifts, color tints, blurred images, distorted depth perception, and restores the fine details of the affected image. [[Website]](https://ieeexplore.ieee.org/abstract/document/10958499)


<!-- <p align="left">
  <img src="https://raw.githubusercontent.com/nsidn98/nsidn98.github.io/master/files/Publications_assets/ObjectRL/006187o.png" width="300"/>
  <img src="https://raw.githubusercontent.com/nsidn98/nsidn98.github.io/master/files/Publications_assets/ObjectRL/006187a.png" width="300"/>
</p>

<p align="left">
  <img src="https://raw.githubusercontent.com/nsidn98/nsidn98.github.io/master/files/Publications_assets/ObjectRL/001902o.png" width="300"/>
  <img src="https://raw.githubusercontent.com/nsidn98/nsidn98.github.io/master/files/Publications_assets/ObjectRL/001902a.png" width="300"/>
</p>

In the above figure, the left column has images from the PascalVOC dataset and the right column has images obtained after the trained RL agent applies the digital transformation (brightness in this case). In the first example, only one person is detected in the input image whereas two persons are detected in the agent-obtained image. Although, the original image looks pleasing to a human eye and the agent-obtained image is quite bright, the agent-obtained image performs better in detection. Similarly, in the second example, the detector misses the group of people in the input image. It detects that group after the agent transformation.
 -->

<!-- Recommended citation: Your Namesdas, You. (2010). "Paper Title Number 2." <i>Journal 1</i>. 1(2). -->

